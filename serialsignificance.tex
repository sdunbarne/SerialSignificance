%%% -*-LaTeX-*-
%%% serialsignificance.tex.orig
%%% Prettyprinted by texpretty lex version 0.02 [21-May-2001]
%%% on Wed Jan  4 08:12:23 2023
%%% for Steve Dunbar (sdunbar@family-desktop)

\documentclass[12pt]{article}

\input{../../../../etc/macros} %\input{../../../../etc/mzlatex_macros}
\input{../../../../etc/pdf_macros}

\bibliographystyle{plain}

\begin{document}

\myheader \mytitle

\hr

\sectiontitle{Serial Chain Significance Testing}

\hr

\usefirefox

\hr

% \visual{Study Tip}{../../../../CommonInformation/Lessons/studytip.png}
% \section*{Study Tip}

% \hr

\visual{Rating}{../../../../CommonInformation/Lessons/rating.png}
\section*{Rating} %one of
% Everyone: contains no mathematics.
% Student: contains scenes of mild algebra or calculus that may require guidance.
% Mathematically Mature: may contain mathematics beyond calculus with proofs.
Mathematicians Only:  prolonged scenes of intense rigor.

\hr

\visual{Section Starter Question}{../../../../CommonInformation/Lessons/question_mark.png}
\section*{Section Starter Question}

Given a statistical test for the null hypothesis that a state from a
Markov chain is ``not unusual'' with respect to some function valuing
the states, what would a \emph{Type I error} for the statistical test
mean?

Given a statistical test for the null hypothesis that a state from a
Markov chain is ``not unusual'' with respect to some function valuing
the states, what would a \emph{Type II error} for the statistical test
mean?

\hr

\visual{Key Concepts}{../../../../CommonInformation/Lessons/keyconcepts.png}
\section*{Key Concepts}

\begin{enumerate}
    \item
        The \( \sqrt{2\epsilon} \) test is:  Observe a trajectory \( X_0,
        X_1, X_2, \dots, X_n \) from the state \( X_0 \sim \pi \) for
        any fixed \( n \).  The event \( v(X_0) \) is an \( \epsilon \)-outlier
        among \( v(X_0), \dots v(X_n ) \) with probability \( p \) which
        is at most \( \sqrt {2\epsilon} \).
    \item
        \begin{theorem}
            \begin{enumerate}
                \item
                    Let \( X \) be a reversible Markov chain on \(
                    \mathcal{X} \).
                \item
                    Let \( v :  \mathcal{X} \to \Reals \) be a value
                    function.
                \item
                    Given \( \sigma_0 \), suppose for a random state \(
                    \sigma \sim \pi \),
                    \[
                        \Prob{v(\sigma) \le v(\sigma_0)} \le \epsilon.
                    \]
            \end{enumerate}
            Then with probability at least
            \[
                \rho \ge 1 - \left( 1 + \frac{\epsilon}{10 \tau_{\text{rel}}}
                \right) \cdot \frac{1}{\sqrt{\pi_{\min}}} \cdot \exp
                \left( \frac {-\epsilon^ {2} \cdot n}{20 \tau_{\text{rel}}}
                \right)
            \] \( v(\sigma) \) is a \( 2\epsilon \)-outlier among \( v(\sigma_0),
            v(\sigma_1), \dots, v(\sigma_n) \) where \( \sigma_0, \sigma_1,
            \dots, \sigma_n \) is a random trajectory starting from \(
            \sigma_0 \).
        \end{theorem}
\end{enumerate}

\hr

\visual{Vocabulary}{../../../../CommonInformation/Lessons/vocabulary.png}
\section*{Vocabulary}
\begin{enumerate}
    \item
        A \defn{value function} or \emph{label} is a function \( v :
        \mathcal{X} \to \Reals \).  The value function has auxiliary
        information about the states of the chain and has no
        relationship to the transition probabilities of the Markov
        chain.
    \item
        A real number \( \alpha_0 \) is an \defn{\( \epsilon \)-outlier}
        among \( \alpha_0, \alpha_1, \dots \alpha_n \) if there are, at
        most, \( \epsilon(n + 1) \) indices \( i \) for which \( \alpha_i
        \le \alpha_0 \), in other words
        \[
            \card{ \setof{\nu}{\alpha_{\nu} \le \alpha_0}} \le \epsilon(n+1).
        \]

    \item
        For \( 1 \le \ell \le n+1 \) say \( \alpha_j \) is \defn{among-\(
        \ell \)-smallest} among \( \alpha_0, \dots \alpha_n \) if there
        are at most \( \ell \) indices from \( 0, \dots, n \) such that
        the value of \( \alpha_i \) is at most the value of \( \alpha_j \).
        That is, \( \alpha_j \) is among-\( \ell \)-smallest among \(
        \alpha_0, \dots \alpha_n \) if \( \card{ \setof{\nu}{\alpha_\nu
        \le \alpha_j}} \le \ell \).  In particular, \( \alpha_j \) is
        among-\( 1 \)-smallest from \( \alpha_0, \dots \alpha_n \) when
        its value is the unique minimum value.
\end{enumerate}

\hr

\section*{Notation}
\begin{enumerate}
    \item
        \( X \), \( X_0 \), \( X_n \) -- Markov chain, starting state,
        general step of the chain.
    \item
        \( \mathcal{X} \) -- State space of a Markov chain
    \item
        \( v :  \mathcal{X} \to \Reals \) -- value or label function
    \item
        \( \pi \) -- stationary distribution
    \item
        \( x_0 \) -- a given state from the state space
    \item
        \( \alpha_0, \alpha_1, \dots \alpha_n \) -- arbitrary real
        numbers for the definition of outliers
    \item
        \( \epsilon \) -- small real number
    \item
        \[
            \rho(j; \ell, n) = \Prob{X_j \text{ is among-\( \ell \)-smallest
            from } X_0, \dots, X_n }
        \]
    \item
        \( \rho(j; 0, n \given \sigma) \) -- the probability \( X_j =
        \sigma \) has the unique minimum value in the Markov chain of
        length \( n \) given the Markov passes through \( \sigma \) at
        step \( j \).
    \item
        \( \xi_j \) -- the indicator variable that is \( 1 \) whenever \(
        X_j \) is among-\( 2\ell \)-smallest among \( X_0, \dots , X_n \)
    \item
        \( p_{\nu}^n \) -- the probability in a \( n \)-step stationary
        trajectory \( X_0, X_1, \dots X_n \), the minimum \( v \) label
        value occurs at \( X_i \).
    \item
        \( N \) -- a large integer
    \item
        \( \zeta_{\nu} = \pm 1 \) -- Bernoulli random variable with
        equal probability \( \frac{1}{2} \)
    \item
        \( \tau_{\text{rel}} \) -- the relaxation time for \( X \) is \(
        1/ (1-\lambda_2) \)
    \item
        \(
        \operatorname{U}
        \) -- uniform distribution on a set
\end{enumerate}

\visual{Mathematical Ideas}{../../../../CommonInformation/Lessons/mathematicalideas.png}
\section*{Mathematical Ideas}

Following the work of Chikina, Frieze, Mattingly and Pegden~%
\cite{doi:10.1080/2330443X.2020.1806763, Chikina2860, Chikina2019} this
section derives statistical tests to detect that a given state from a
stationary distribution of a reversible Markov chain is unusual.  The
\emph{research hypothesis} is:
\begin{quote}
    The given state is unusual with respect to some value measure.
\end{quote}
Using a value function on the states of the Markov chain, this section
rigorously bounds the probability that the given state is an outlier
among the values.  Doing so gives a \( p \) value for outliers, assuming
the given state comes from the stationary distribution of the chain.
The \emph{null hypothesis} is:
\begin{quote}
    The given state is not unusual with respect to some value measure.
\end{quote}

In simplest form, the test is the following:  Given a particular state
in the Markov chain, take a random walk from the state for any number of
steps.  The simplest statistical test is finding the value of the
presented state is an \( \epsilon \)-outlier on the walk with
probability \( p \le \sqrt{2\epsilon} \) assuming the state was from the
stationary distribution.  The test assumes nothing about the Markov
chain beyond reversibility.  With the simple serial test, probability at
\( p \approx \sqrt{\epsilon} \) is the best possible in general.
Compound tests in later sections using multiple random walks give
improved probability of observing outlier values with \( p \approx
\epsilon \).

\subsection*{Background}

An essential problem in statistics is to bound the probability of a
surprising observation under the hypothesis that observations are drawn
from some probability distribution.  Often the bounding calculation is
difficult.  First, defining the way in which the outcome is surprising
requires care.  Second, the correct choice of the distribution implied
by the hypothesis may not be immediately clear.  Third, the \( p \)
value calculations may be difficult even when the observation is
surprising in a simple way and the hypothesis distribution is known but
where there is no simple algorithm to draw samples from this
distribution.  In the third case, the best candidate method to sample
under the hypothesis is often through a Markov chain, which essentially
takes a long random walk on the possible values of the distribution.
Under suitable conditions, theorems about Markov chains guarantee the
chain converges to its stationary distribution, sometimes at a known
rate, allowing a random sample to be drawn from a distribution
quantifiably close to the target distribution.  This principle is behind
Markov chain Monte Carlo sampling and statistical physics models.

A problem in applications of Markov chains is the usually unknown rate
at which the chain converges to the stationary distribution.  It is rare
to have rigorous results on the mixing time of a real-world Markov
chain.  In practice, sampling proceeds by running a Markov chain for a
``long time'' and hoping enough mixing has occurred.

In this section, the problem is assessing statistical significance in a
Markov chain \emph{without requiring information on the mixing time} of
the chain or indeed, any special structure at all in the chain beyond
reversibility.  Consider a reversible Markov chain \( X \) on a state
space \( \mathcal{X} \) with an associated value function \( v :
\mathcal{X} \to \Reals \) (The notation here is \( v \) for the \emph{value}
function instead of \( \omega \) as in~%
\cite{Chikina2860}, to avoid overloading notation.  Here \( \omega \)
represents sample points in the sample space.) The values give auxiliary
information without any relationship to the transition probabilities of \(
X \).  The goal is to show a given state \( x_0 \) is unusual for states
drawn from the stationary distribution \( \pi \).  With good bounds on
the mixing time of \( X \), sampling \( v(x) \) from a distribution \(
\pi \) gives a rigorous \( p \) value for the probability of the
smallness of \( v(x_0) \).  However, such convergence bounds are rarely
available.

Proposition~%
\ref{thm:serialsignificance:basethm} detects that \( X_0 = x_0 \) is
unusual relative to states chosen randomly according to \( \pi \).  The
test does not need bounds on the mixing rate of \( X \).  First, a more
precise definition of ``unusual relative to states chosen randomly''.

\begin{definition}
    A \defn{value function},~%
    \index{value function}
    also called a \emph{label}, is a function \( v :  \mathcal{X} \to
    \Reals \).  The value function has auxiliary information about the
    states of the chain and has no relationship to the transition
    probabilities of the Markov chain.
\end{definition}

\begin{remark}
    For the statistical significance tests in this section the only
    relevant feature of the value function is the ranking it imposes on
    the elements of \( \mathcal{X} \).
\end{remark}

\subsection*{Outliers}

\begin{definition}
    A real number \( \alpha_0 \) is an \defn{\( \epsilon \)-outlier}%
    \index{\( \epsilon \)-outlier}
    among \( \alpha_0, \alpha_1, \dots \alpha_n \) (not necessarily in
    increasing order and with repetitions possible) if there are, at
    most, \( \epsilon(n + 1) \) indices \( i \) for which \( \alpha_i
    \le \alpha_0 \), in other words \( \card{%
    \setof{\nu}{\alpha_{\nu} \le \alpha_0}} \le \epsilon(n+1) \).
    Alternatively, \( \alpha_0 \) is in the smallest \( \epsilon \)-fraction
    of the \( n + 1 \) ranked values of \( \alpha_0, \alpha_1, \dots
    \alpha_n \).
\end{definition}

\begin{remark}
    For \( \alpha_0 \) to be an \( \epsilon \)-outlier says \( \alpha \)
    is relatively more rare than the \( \epsilon \)-proportion that
    might be expected if the indices were uniformly distributed among
    the range of ranked values.  For example, consider the set of \( 11 \)
    values
    \begin{multline*}
        \alpha_0 = 3, \alpha_1 = 1, \alpha_2 = 4, \alpha_3 = 1, \alpha_4
        = 5, \alpha_5 = 9, \\
        \alpha_6 = 2, \alpha_7 = 6, \alpha_8 = 5, \alpha_9 = 3, \alpha_{10}
        = 5.
    \end{multline*}
    Then \( \alpha_0 = 3 \) is \emph{not} an \( 0.1 \)-outlier since \(
    5 = \card{%
    \setof{\nu}{\alpha_\nu \le \alpha_0}} \ge \epsilon(n+1) = 1.1 \). In
    fact, \( \alpha_0 = 3 \) \emph{is not} a \( 0.2 \)-outlier (the
    cardinality is \( 5 \), greater than \( 2.2 \)) or even a \( 0.3 \)-outlier.
    This example says \( \alpha_0 = 3 \) is not rare among small
    fractions of the values.
\end{remark}

The following definition uses counting instead of proportions to express
the smallness of a real number relative to a set.  The proof of
Proposition~%
\ref{thm:serialsignificance:basethm} uses this definition.

\begin{definition}
    For \( 1 \le \ell \le n+1 \) say \( \alpha_j \) is \defn{among-\(
    \ell \)-smallest}%
    \index{among-\( \ell \)-smallest}
    among \( \alpha_0, \dots \alpha_n \) if there are at most \( \ell \)
    indices from \( 0, \dots, n \) such that \( \alpha_{\nu} \) is at
    most as large as \( \alpha_j \).  That is, \( \alpha_j \) is among-\(
    \ell \)-smallest among \( \alpha_0, \dots \alpha_n \) if
    \[
        \card{ \setof{\nu}{\alpha_{\nu} \le \alpha_j}} \le \ell.
    \] In particular, \( \alpha_j \) is among-\( 1 \)-smallest from \(
    \alpha_0, \dots \alpha_n \) when its value is the unique minimum
    value.
\end{definition}

\begin{remark}
    The definitions are connected:
    \begin{itemize}
        \item
            If \( \alpha_0 \) is an \( \epsilon \)-outlier, then \(
            \alpha_0 \) is among-\( \lfloor \epsilon (n+1) \rfloor \)-smallest.
        \item
            If \( \alpha_0 \) is among-\( \ell \)-smallest, then \(
            \alpha_0 \) is an \( \frac{\ell}{n+1} \)-outlier.
    \end{itemize}
\end{remark}

\begin{remark}
  The definition of among-\( \ell \)-smallest differs from the
  definition of \(\ell\)-small in~\cite{Chikina2860}.  Their
  definition excludes \( \alpha_j \) from the count, while the
  definition here includes \( \alpha_j \) in the count.  The
  definition here corresponds with the definition of \( \epsilon
  \)-outlier which includes the value in question in the count.
\end{remark}

\begin{example}
    Consider the set of \( 11 \) real numbers drawn at random uniformly
    from \( [0,1] \) and presented in sorted order:
    \[
        0.006, 0.042, 0.359, 0.381, 0.399, 0.523, 0.533, 0.702, 0.762,
        0.921, 0.963
    \] so \( n = 10 \) and let \( \epsilon = 0.1 \).

    Then \( 0.006 \) is an \( 0.1 \)-outlier since the number of indices
    for which \( \alpha_i \le 0.006 \), specifically \( 1 \), is less
    than \( \epsilon(n+1) = 0.1\cdot (10 +1) = 1.1 \).  See Figure~%
    \ref{fig:serialsignificance:epsoutlier} for an illustration of this
    example.  However, \( 0.006 \) is not an \( 0.05 \)-outlier since
    the number of indices for which \( \alpha_i \le 0.006 \),
    specifically \( 1 \), is more than \( \epsilon(n+1) = 0.05 \cdot (10
    +1) = 0.55 \).  As a further example, there are \( 2 \) indices for
    which \( \alpha_i \le 0.042 \) so \( 0.042 \) is not an \( 0.05 \), \(
    0.1 \) or \( 0.15 \)-outlier, but it is a \( 0.2 \)-outlier

    \begin{figure}
        \centering
\begin{asy}
            size(5inches);

            real myfontsize = 12; real mylineskip = 1.2*myfontsize; pen
            mypen = fontsize(myfontsize, mylineskip); defaultpen(mypen);

            real eps = 0.1; pair[] alpha = {(0.006,0), (0.042,0), (0.359,0),
            (0.381,0), (0.399,0), (0.523,0), (0.533,0), (0.702,0), (0.762,0),
            (0.921,0), (0.963,0)}; pair h = (0,0.05);

            draw((0,0)--(1,0)); for(int i=0; i<11; ++i) { dot(alpha[i]);
            } draw(circle(alpha[0], 0.01), red);

            draw( (alpha[0]+h)--(alpha[10]+h), Bars); draw( (alpha[0]+h)--
            (eps * (alpha[10]-alpha[0]) + h), Bars);

            label("\( \epsilon \)", ((1/2)* eps * (alpha[10]-alpha[0]))
            + 1.5*h); label("\( \alpha_0 \)", alpha[0]-h); label("\(
            \alpha_n \)", alpha[10]-h);
\end{asy}
        \caption{Example of \( \epsilon \)-outliers with \( 11 \) random
        values in \( [0,1] \) and \( \epsilon = 0.1 \).}%
        \label{fig:serialsignificance:epsoutlier}
    \end{figure}

    Also, \( 0.006 \) is among-\( 1 \)-smallest since there is \( 1 \)
    index \( i \ne j \) from \( 0, \dots, 10 \) such that the value of \(
    \alpha_i \) is at most the value of \( 0.006 \).  Additionally, \(
    0.042 \) is among-\( 2 \)-smallest since there is \( 2 \) indices
    from \( 0, \dots, 10 \) such that the value of \( \alpha_i \) is at
    most the value of \( 0.042 \).

    The notion of \( \epsilon \)-outlier is a generalization of median,
    quartile and percentile, see the exercises.  The exercises also have
    a comparison of \( \epsilon \)-outlier and among-\( \ell \)-smallest
    to the notions of the order of a list and the rank of elements of a
    list.
\end{example}

\subsection*{Probability of Outliers}

Proposition~%
\ref{thm:serialsignificance:basethm} gives an upper bound on the
probability of a value of a Markov chain being among-\( \ell \)-smallest
using the following definition.
\begin{definition}
    Let \( n \) be fixed.  Let \( X_1, X_2, \dots, X_n \) be a sample of
    the Markov chain starting from \( X_0 = \sigma_0 \). Suppose the
    state space has a value function \( v : \mathcal {X} \to \Reals \).
    For \( 0 \le j \le n \) define
    \[
        \rho(j; \ell, n) = \Prob{X_j \text{ is among-\( \ell \)-smallest
        from } X_0, \dots, X_n }
    \] and
    \[
        \rho(j; \ell, n \given \sigma) = \Prob{X_j \text{ is among-\(
        \ell \)-smallest from } X_0, \dots, X_n%
        \given X_j = \sigma }.
    \]
\end{definition}

Observe \( \rho(j; 1 ,n) \) is the probability \( X_j \) has the unique
minimum value in the Markov chain of length \( n \).  Likewise, \( \rho(j;
1, n \given \sigma) \) is the probability \( X_j = \sigma \) has the
unique minimum value in the Markov chain of length \( n \) given that
the Markov passes through \( \sigma \) at step \( j \).  Additionally, \(
\rho(0; \ell, n\given \sigma_0) \) is the probability that the starting
value \( X_0 = \sigma_0 \) is among-\( \ell \)-smallest from \( v(X_0) \)
through \( v(X_n) \).

\begin{example}
    The previous definitions have several levels:
    \begin{itemize}
        \item
            Markov chain sample paths of some length \( n \) with a
            value function,
        \item
            a upper bound \( \ell \),
        \item
            a specific step \( j \),
        \item
            the number of steps with value less than the given step, and
        \item
            the probability that number of steps is at most the upper
            bound.
    \end{itemize}

    This example illustrates those concepts of \emph{among-\( \ell \)-smallest}
    and \( \rho(j; 0 ,n) \) in the context of a specific Markov-chain.

    Consider the alternative Ehrenfest urn model.%
    \index{Ehrenfest urn model}
    Two urns, labeled \( A \) and \( B \), contain a total of \( N = 7 \)
    balls.  At each step a ball is selected at random with all
    selections equally likely.  Then an urn is selected, urn \( A \)
    with probability \( p = \frac{1}{2} \) and urn \( B \) with
    probability \( q = 1-p = \frac {1}{2} \) and the ball is moved to
    that urn.  The state at each step is the number of balls in the urn \(
    A \), from \( 0 \) to \( N \).  Start from state \( \sigma \)
    selected at random from the stationary distribution. For this simple
    example, let the value function be the number of balls in urn \( A \),
    so that the value function is identical with the state number.  The
    simplicity of the alternative Ehrenfest urn model limits the
    possibilities for value functions.

    This example uses the reproducible simulation of the alternate
    Ehrenfest urn model in \texttt{simRhoProb.R}. The path length, or
    number of steps, is \( 10 \), short enough to be comprehensible,
    long enough have some content.  Including the start state, the
    Markov chain has \( 11 \) values. The number of trials is \( 15 \),
    enough to illustrate the variety of sample paths, but not too many
    to overwhelm. Thus the Markov chain is simulated \( 15 \) times
    starting from a state selected at random from the stable
    distribution.  The values along the paths are recorded in the rows
    of an \( 15 \times 11 \) matrix.  See Table~%
    \ref{tab:serialsignificance:recordpaths}. Then for each given step \(
    j \) along the path, the count of other step values along the path
    less than or equal to the given step value is compared to \( \ell \).
    If the count is less than or equal to \( \ell \), that makes the
    given step value at \( j \) among-\( \ell \)-smallest and the count
    in a \( 11 \times 11 \) matrix is incremented.  See Table~%
    \ref{tab:serialsignificance:rhon}.  When done, divide the counting
    matrix entries by the number of trials, \( 15 \), to give an
    empirical estimate of \( \rho(j; \ell ,n) \) with \( n = 11 \).

    \begin{table}
        \centering
        \begin{tabular}{c|ccccccccccc}
            step   & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ 
            \hline
            1      & 5 & 4 & 4 & 3 & 3 & 4 & 4 & 4 & 4 & 3 & 3  \\ 
            2      & 2 & 2 & 3 & 4 & 4 & 3 & 4 & 3 & 2 & 3 & 4  \\ 
            3      & 3 & 3 & 3 & 2 & 3 & 2 & 2 & 3 & 3 & 4 & 4  \\ 
            4      & 3 & 3 & 3 & 2 & 3 & 2 & 3 & 4 & 4 & 3 & 4  \\ 
            5      & 5 & 5 & 6 & 5 & 5 & 4 & 4 & 4 & 3 & 3 & 4  \\ 
            6      & 4 & 4 & 4 & 4 & 4 & 3 & 4 & 5 & 4 & 4 & 5  \\ 
            7      & 3 & 3 & 3 & 4 & 5 & 6 & 5 & 4 & 3 & 3 & 4  \\ 
            8      & 4 & 4 & 4 & 4 & 4 & 5 & 4 & 4 & 4 & 4 & 4  \\ 
            9      & 4 & 5 & 5 & 5 & 4 & 4 & 4 & 4 & 4 & 5 & 4  \\ 
            10     & 4 & 5 & 5 & 4 & 5 & 4 & 4 & 5 & 5 & 4 & 5  \\ 
            11     & 4 & 4 & 3 & 3 & 2 & 3 & 3 & 4 & 3 & 2 & 2  \\ 
            12     & 3 & 3 & 3 & 3 & 2 & 3 & 3 & 3 & 2 & 2 & 3  \\ 
            13     & 2 & 3 & 2 & 2 & 1 & 0 & 1 & 2 & 2 & 3 & 2  \\ 
            14     & 1 & 2 & 3 & 4 & 5 & 6 & 6 & 6 & 5 & 4 & 5  \\ 
            15     & 5 & 6 & 7 & 6 & 5 & 4 & 4 & 5 & 6 & 6 & 5  \\ 
        \end{tabular}
        \caption{Record of 15 sample paths of the alternate Ehrenfest
        urn model.}%
        \label{tab:serialsignificance:recordpaths}
    \end{table}

    \begin{table}
        \centering
        \begin{tabular}{c|ccccccccccc}
                   & 1 & 2 & 3 & 4 & 5 & 6 & 7  & 8  & 9  & 10 & 11 \\ 
            \hline
            0      & 1 & 1 & 2 & 2 & 4 & 5 & 6 & 7  & 10 & 12 & 15 \\ 
            1      & 0 & 1 & 2 & 2 & 3 & 3 & 3 & 4  & 6  & 10 & 15 \\ 
            2      & 0 & 0 & 1 & 1 & 2 & 2 & 3 & 5  & 8  & 10 & 15 \\ 
            3      & 0 & 1 & 2 & 3 & 5 & 5 & 5 & 7  & 9  & 12 & 15 \\ 
            4      & 0 & 0 & 3 & 4 & 4 & 5 & 6 & 8  & 10 & 13 & 15 \\ 
            5      & 2 & 4 & 5 & 5 & 6 & 7 & 9 & 10 & 10 & 11 & 15 \\ 
            6      & 0 & 1 & 3 & 3 & 4 & 5 & 6 & 8  & 9  & 12 & 15 \\ 
            7      & 0 & 0 & 0 & 0 & 0 & 2 & 4 & 5  & 7  & 9  & 15 \\ 
            8      & 0 & 1 & 3 & 3 & 4 & 4 & 5 & 7  & 10 & 13 & 15 \\ 
            9      & 0 & 1 & 3 & 4 & 7 & 7 & 8 & 9  & 10 & 12 & 15 \\ 
            10     & 0 & 0 & 1 & 2 & 2 & 4 & 5 & 7  & 8  & 9  & 15 \\ 
        \end{tabular}
        \caption{The count of step values \( \)}%
        \label{tab:serialsignificance:rhon}
    \end{table}

    Next is a detailed analysis of Table~%
    \ref{tab:serialsignificance:recordpaths} to illustrate the entries
    of the counting matrix in Table~%
    \ref{tab:serialsignificance:rhon}.
    \begin{itemize}
        \item
            The entry at \( [0,1] \) records the number of trials where
            the starting value \( j=0 \) is the unique minimum.  In
            trials \( 2 \), \( 7 \), \( 8 \), \( 9 \), \( 10 \) the
            start value is the minimum of the path, but not the unique
            minimum. In trial 14, the start value \( 1 \) is the unique
            minimum, which gives the entry value \( 1 \).
        \item
            Entries at \( [j,1] \), with \( j \ge 1 \), record the
            number of trials where the value of step \( X_j \) is the
            unique minimum.  In trial 6, at step \( j=5 \) and in trial
            13, at step \( j = 5 \) the unique minimum occurs. Thus the \(
            [5,0] \) entry is \( 2 \).
        \item
            The entry at \( [0,1] \) records the number of trials where
            the start value, namely \( X_0 \), is among-\( 1 \)-smallest.
            That is, the number of trials where the number of values
            less than or equal to the start value is less than or equal
            to \( 1 \).  This is just trial 14 again.
        \item
            The entry at \( [0,2] \) records the number of trials where
            the start value \( X_0 \) is among-\( 2 \)-smallest.  That
            is, the number of trials where the number of values less
            than or equal to the start value is less than or equal to \(
            2 \).  This is just trial 14 again, with no other values
            less than \( X_0 = 1 \), and trial 2 with \( 2 \) step
            values, namely \( X_1 \) and \( X_8 \) equal to \( X_0 = 2 \).
    \end{itemize}
    See the exercises for more explanation of entries in Table~%
    \ref{tab:serialsignificance:rhon}.

    This example is deliberately small in scope to illustrate the
    definition of \( \rho(j; \ell, n) \).  Increasing the path length
    and the number of trials gives better empirical estimates for \(
    \rho(j; \ell, n) \).
\end{example}

\begin{remark}
    For a given value of \( n \), \( \rho(j; \ell, n) \) is increasing
    in \( \ell \) like a cdf.  This is an obvious conclusion, since the
    definition is in terms of an inequality and gets more permissive in
    terms of the number of allowed values in the among-\( \ell \)-smallest.
\end{remark}

\begin{proposition}
    \label{thm:serialsignificance:basethm}
    \begin{enumerate}
        \item
            Fix \( n \).
        \item
            Suppose \( \sigma_0 \) is chosen from the stationary
            distribution \( \pi \) of the reversible Markov chain \( X \).
        \item
            Suppose \( X_1, X_2, \dots, X_n \) is a sample of the Markov
            chain starting from \( X_0 = \sigma_0 \).
        \item
            Suppose the state space has a value function \( v : \mathcal
            {X} \to \Reals \).
        \item
            Let
            \[
                \rho(j; \ell, n) = \Prob{v(X_j) \text{ is among-\( \ell \)-smallest
                from } v(X_0), \dots, v(X_n) }.
            \]
    \end{enumerate}

    Then
    \[
        \rho(0; \ell, n) \le \sqrt{ \frac{2\ell + 1}{n+1}}.
    \]
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item
            Let \( \pi \) denote the stationary distribution for \( X \)
            and suppose the initial state \( X_0 \) is distributed as \(
            X_0 \sim \pi \), so that in fact, \( X_\nu \sim \pi \) for
            all \( \nu \).
        \item
            Consider the set of values, \( v(X_0), v(X_1), \dots, v(X_n)
            \).  For simplicity, say \( X_j \) is among-\( \ell \)-smallest
            among \( X_0, \dots, X_n \) instead of referring to the
            values of the states.  For a first understanding of the
            proof, use the \( \ell = 1 \) case, so the minimum value is
            the focus. % \item
            %     For \( 0 \le j \le n \) define
            %     \[
            %         \rho(j; \ell, n) = \Prob{X_j \text{ is among- \(\ell\)
            %         -smallest from } X_0, \dots, X_n }
            %     \] and
            %     \[
            %         \rho(j; \ell, n \given \sigma) = \Prob{X_j \text{ is
            %         among- \(\ell\) -smallest from } X_0, \dots, X_n%
            %         \given X_j = \sigma }.
            %     \] Observe \( \rho(j; 0 ,n) \) is the probability \( X_j \)
            %     has the unique minimum value in the Markov chain of length \(
            %     n \).  Likewise, \( \rho(j; 0, n \given \sigma) \) is the
            %     probability \( X_j = \sigma \) has the unique minimum value
            %     in the Markov chain of length \( n \) given that the Markov
            %     passes through \( \sigma \) at step \( j \).
        \item
            Observe that because \( X_s \sim \pi \) for all \( s \),
            additionally
            \begin{multline*}
                \rho(j; \ell, n \given \sigma) =\\
                \Prob{X_{s+j} \text{ is among-\( \ell \)-smallest among
                } X_s, \dots, X_{s+n} \given X_j = \sigma }.
            \end{multline*}
        \item
            \label{enum:serialsignificance:basethm5} Because \( X = X_0
            , X_1, \dots \) is stationary and reversible, the
            probability of \( (X_0, \dots, X_n ) = (\sigma_0, \dots ,
            \sigma_n ) \) equals the probability \( (X_0,\dots, X_n ) =
            (\sigma_n, \dots, \sigma_0) \) for any fixed sequence \( (\sigma_0,
            \dots, \sigma_n ) \).  Thus, any sequence \( (\sigma_0,\dots,
            \sigma_n ) \) for which \( \sigma_j = \sigma \) and \(
            \sigma_j \) is a among-\( \ell \)-smallest corresponds to an
            equiprobable sequence \( (\sigma_n,\dots, \sigma_0 ) \), for
            which \( \sigma_{n-j} = \sigma \) and \( \sigma_{n-j} \) is
            among-\( \ell \)-smallest.  Thus a first fact about \( \rho(j;
            \ell, n \given \sigma) \) for reversible Markov chains is:
            \[
                \rho(j; \ell, n \given \sigma) = \rho(n-j; \ell, n
                \given \sigma).
            \]
        \item
            \label{enum:serialsignificance:basethm6} The next claim is:
            \[
                \rho(j; 2\ell, n \given \sigma) \ge \rho(j; \ell, j
                \given \sigma) \cdot \rho(0; \ell, n-j \given \sigma).
            \] The proof of the claim is to consider the subevent \( X_j
            \) is among-\( \ell \)-smallest among \( X_0, \dots, X_j \)
            and also among-\( \ell \)-smallest among \( X_j, \dots, X_n \).
            These events are conditionally independent when conditioning
            on the value of \( X_j = \sigma \), and \( \rho(j; \ell, j
            \given \sigma) \) gives the probability of the first of
            these events.  The second probability \( \rho(0; \ell, n-j
            \given \sigma) \) gives the probability of the second event.
            Finally, as a subevent when both of these events happen, \(
            X_j \) is among-\( 2\ell \)-smallest from \( X_0, \dots, X_n
            \).  This establishes the inequality in the claim.
        \item
            \label{enum:serialsignificance:basethm7} Using the previous
            steps
            \begin{align*}
                \rho(j; 2\ell, n \given \sigma) &\ge \rho(j; \ell, j
                \given \sigma) \cdot \rho(0; \ell, n-j \given \sigma) \\
                &= \rho(0; \ell, j \given \sigma) \cdot \rho(0; \ell,
                n-j \given \sigma) \\
                &\ge \left( \rho(0; \ell, n \given \sigma) \right)^2.
            \end{align*}
            where the first inequality comes from step~%
            \ref{enum:serialsignificance:basethm6}.  The middle equality
            comes from step~%
            \ref{enum:serialsignificance:basethm5}.  The last inequality
            comes from the simple observation that \( \rho(j; \ell, n
            \given \sigma) \) is nonincreasing in \( n \) for fixed \( j
            \), \( \ell \) and \( \sigma \), in particular for \( j \)
            fixed to be \( 0 \).
        \item
            \label{enum:serialsignificance:basethm8} Now \( \rho(j;\ell,n)
            = \E{\rho(j; \ell, n \given X_j)} \), where the expectation
            is taken over the random choice of \( X_j \sim \pi \). Thus,
            taking expectations over the inequality in step~%
            \ref{enum:serialsignificance:basethm7},
            \begin{multline}
                \rho(j; 2\ell, n) = \E{\rho(j; 2\ell, n \given \sigma)}
                \ge \E{\rho(j; \ell, n \given \sigma)^2} \\
                \ge \left( \E{\rho(0, \ell, n \given \sigma)}\right)^2 =
                \left( \rho(0; \ell, n) \right)^2
            \end{multline}
            where the second of the two inequalities is the
            Cauchy-Schwartz inequality.
        \item
            To complete the proof, sum the left- and right-hand sides of
            the inequality in step~%
            \ref{enum:serialsignificance:basethm8}
            \[
                \sum\limits_{j=0}^n \rho(j; 2\ell, n) \ge (n+1) (\rho(0;
                \ell, n))^2.
            \]
        \item
            Letting \( \xi_j \), \( 0 \le i \le n \) be the indicator
            variable that is \( 1 \) whenever \( X_j \) is among-\( 2\ell
            \)-smallest among \( X_0, \dots , X_n \), then \( \sum_{\nu=0}^n
            \xi_{\nu} \) is the number of among-\( 2\ell \)-smallest
            terms which is at most \( 2\ell + 1 \).  Therefore,
            linearity of expectation gives
            \[
                2\ell + 1 \ge (n + 1)(\rho(0; l,n) )^2
            \] so
            \[
                \rho(0; \ell, n) \le \sqrt{ \frac{2\ell + 1}{n+1}}.
            \]
    \end{enumerate}
\end{proof}

\begin{remark}
    Proposition~%
    \ref{thm:serialsignificance:basethm} only gives a theoretical upper
    bound for \( \rho(0; \ell, n) \) although the alternate Ehrenfest
    urn model simulation gives empirical estimates for all \( \rho(j;
    \ell, n) \).  The theoretical upper bound is not very sharp either,
    for instance \( \rho(0; n, n) = 1 \) for all \( j \), but the
    theoretical upper bound is approximately \( \sqrt{2} \).

\end{remark}

\begin{remark}
    Proposition~%
    \ref{thm:serialsignificance:basethm} can only show a given sample is
    unusual with respect to \( \pi \).  The Proposition does not give a
    way to generate samples from \( \pi \).
\end{remark}

\begin{remark}
    The Proposition makes no assumptions on the structure of the Markov
    chain beyond reversibility.  If a Markov chain can be broken down
    into two disjoint communicating classes, i.e.\ two disconnected
    Markov chains and if the individual Markov chains are reversible,
    then so is the full chain.  This is a worthwhile observation, since
    all examples of reversible Markov chains so far are irreducible,
    giving the wrong impression that all reversible Markov chains are
    irreducible.

    In particular, the Proposition applies even if the chain is not
    irreducible, i.e.\ even if the state space is not connected under
    the Markov chain.  In this disconnected case the chain will never
    completely mix since it remains in one component.  This remark is
    important for the example application below of Markov chains
    sampling valid political districting, where it is not known that the
    Markov chains are irreducible.  Even if valid districting only
    requires contiguous districts of roughly equal populations, it is
    unknown if any valid districting is reachable from any other by a
    legal sequence of steps.

    This suggests a careful choice of value function can help to
    distinguish irreducible components, especially in political
    districting.  Suppose one component is relatively small and unusual
    compared to more typical states from a larger component.  A value
    function measuring this difference would be a candidate for the
    Proposition.
\end{remark}

\begin{corollary}[The \( \sqrt{2\epsilon} \) Test]
    \label{thm:serialsignificance:sqrtepstest}
    \begin{enumerate}
        \item
            Fix \( n \).
        \item
            Suppose \( \sigma_0 \) is chosen from the stationary
            distribution \( \pi \) of the reversible Markov chain \( X \).
        \item
            Suppose \( X_1, X_2, \dots, X_n \) is a sample of the Markov
            chain starting from \( X_0 = \sigma_0 \).
        \item
            Suppose the state space has a value function \( v : \mathcal
            {X} \to \Reals \).
    \end{enumerate}
    Then the event \( v(X_0) \) is an \( \epsilon \)-outlier among \( v(x_0),
    \dots v(x_n ) \) with probability \( p = \sqrt{2\epsilon} \).
    % under the null hypothesis that \( X_0 \sim \pi \).
\end{corollary}
\index{\(\sqrt{2\epsilon}\)-test}

\begin{proof}
    \begin{enumerate}
        \item
            Let the reversible Markov chain \( X \), the stationary
            distribution \( \pi \), the value function \( v \), fixed
            integer \( n \), and fixed small proportion \( \epsilon \)
            determine a \( n \)-vector \( (\rho(0; \epsilon, n), \rho(1;
            \epsilon, n), \dots, \rho(n; \epsilon, n)) \) where, with a
            slight abuse of notation, for each \( i \), \( \rho(i;
            \epsilon, n) \) is the probability that in a \( n \)-step
            stationary trajectory \( X_0, X_1, \dots X_n \), \( v(X_i) \)
            is among-\( \lfloor \epsilon(n+1) \rfloor \)-smallest in the
            list \( v(X_0), v(X_1), \dots, v(X_n) \).
        \item
            Proposition~%
            \ref{thm:serialsignificance:basethm} proves
            \[
                \rho(0; \epsilon, n) \le \sqrt{\frac{2 \lfloor \epsilon
                (n+1) \rfloor}{n+1}} \le \sqrt{\frac{2 \epsilon (n+1)}{n+1}}
                = \sqrt{2\epsilon}.
            \]
    \end{enumerate}
\end{proof}

\begin{remark}
    This is a restatement of Proposition~%
    \ref{thm:serialsignificance:basethm} as a statistical test in terms
    of the proportion \( \epsilon \).

\end{remark}

\begin{remark}
    Chikina, et al.~%
    \cite{Chikina2860} call Corollary~%
    \ref{thm:serialsignificance:sqrtepstest} the \( \sqrt{\epsilon} \)
    test, but this section slightly revises their terminology.
    Corollary~%
    \ref{thm:serialsignificance:sqrtepstest} could be called either the \(
    \epsilon \) test, referring to the outlier fraction, or the \( \sqrt
    {2 \epsilon} \) test referring to the significance level.  Looking
    ahead to more refined tests, this section calls Corollary~%
    \ref{thm:serialsignificance:sqrtepstest} the \( \sqrt{2 \epsilon} \)
    test for serial or sequential Markov chain samples.
\end{remark}

\begin{remark}
    The Type I error rate.%
    \index{Type I error rate}
    is the probability of rejecting the null hypothesis, given the null
    hypothesis is true.  See Figure~%
    \ref{fig:serialsignificance:errortypes} for a diagram of possible
    decisions about the null hypothesis and possible errors in the
    decision.  Corollary~%
    \ref{thm:serialsignificance:sqrtepstest} says if the value is an \(
    \epsilon \)-outlier, then the probability of \emph{erroneously}
    concluding \( X_0 \) is \emph{not unusual}, when in fact \( X_0 \)
    \emph{is} unusual, is less than \( \sqrt{2 \epsilon } \).

    A subsection later shows the relationship \( p \approx \sqrt{\epsilon}
    \) is best possible.  It is currently an open question whether the
    constant \( 2 \) can be improved.
\end{remark}

\begin{remark}
    The power%
    \index{power}
    of a binary null hypothesis test is the probability the test
    correctly rejects the null hypothesis when the alternative
    hypothesis is true.  The power is the complementary probability of a
    Type II error.  Here the power is the probability the \( \sqrt{2\epsilon}
    \) Test correctly says \( X_0 \) as an \( \epsilon \)-outlier is
    unusual.  Later subsections consider the statistical power of the \(
    \sqrt{2\epsilon} \) test.
\end{remark}

\begin{figure}
    \centering
\begin{asy}
        settings.outformat = "pdf";

        // import graph;

        size(5inches);

        real myfontsize = 12; real mylineskip = 1.2*myfontsize; pen
        mypen = fontsize(myfontsize, mylineskip); defaultpen(mypen);

        real w = 20; real w1 = 0.2 * w; real w2 = 0.4 * w; real w3 = 0.7
        * w;

        real h = 20; real h1 = 0.4 * h; real h2 = 0.8 * h; real h3 = 0.9
        * h;

        draw( (0,0)--(w,0)--(w,h)--(0,h)--cycle);

        fill( (0,0)--(w2,0)--(w2,h2)--(w,h2)--(w,h)--(0,h)--cycle,
        lightgray);

        fill( (w3,h1)--(w,h1)--(w,h2)--(w3,h2)--cycle, lightred+opacity(0.5));
        fill( (w2,0)--(w3,0)--(w3,h1)--(w2,h1)--cycle, lightblue+opacity
        (0.5));

        draw( (w1, 0)--(w1, h2) ); draw( (w2, 0)--(w2, h)); draw( (w3, 0)--
        (w3, h3));

        draw( (w1, h1)--(w, h1)); draw( (0, h2)--(w, h2)); draw( (w2, h3)--
        (w, h3));

        label("Error Types", (w2/2, (h2+h)/2)); label("Null hypothesis
        is", ( (w2 + w)/2, (h3 + h)/2 ) ); label("True", ( (w2+w3)/2, (h2+h3)/2
        )); label("False", ( (w3+w)/2, (h2+h3)/2 )); label("Decision", (w1/2,
        h2/2) ); label("Don't reject", ( (w1+w2)/2, (h1+h2)/2) ); label("Reject",
        ( (w1+w2)/2, h1/2 )); label("Correct Inference", ((w2+w3)/2, (h1+h2)/2
        ) ); label("Type II error", ( (w3+w)/2, (h1+h2)/2 )); label("Correct
        Inference", ( (w3+w)/2, h1/2 ) ); label("Type I error", ( (w2+w3)/2,
        h1/2));
\end{asy}
    \caption{Tabular presentation of hypothesis testing, decisions and
    error types.}%
    \label{fig:serialsignificance:errortypes}
\end{figure}

Another way to look at the \( \sqrt{2\epsilon} \) Test is the following.
The Markov chain \( X \), the stationary distribution \( \pi \), the
valuing \( v \), and a fixed integer \( n \) determine a \( n \)-vector \(
(p_0^n, p_1^n, \dots, p_n^n) \) where for each \( \nu \), \( p_{\nu}^n \)
is the probability that in a \( n \)-step stationary trajectory \( X_0,
X_1, \dots X_n \), the minimum \( v \) value occurs at \( X_i \).  In
other words, choosing \( X_0 \) randomly from the stationary
distribution \( \pi \) and taking \( n \) steps in \( \mathcal{X} \) to
obtain the trajectory \( X_0, X_1, \dots X_n \), the probability that
observing \( v(X_i) \) is the minimum among \( v(X_0), v(X_1), \dots, v(X_n)
\) is \( p_i^n \).  Adopt the convention that ties for minimum among the
values \( v (X_0), v (X_1), \dots, v(X_n) \) picks \( i \) randomly (uniformly
among the steps achieving the minimum) so \( p_0^n + p_1^n + \cdots + p_n^n
= 1 \) for \( X \), \( \pi \), \( n \).

Thinking the minimum value is roughly equally likely to occur anywhere
in the chain, it might be natural to guess \( p_i^n \approx \frac{1}{n+1}
\).  To be the minimum value means the value occurs in the least \(
\frac{1}{n+1} \) fraction, so \( \epsilon = \frac {1}{n+1} \).
Corollary~%
\ref{thm:serialsignificance:sqrtepstest} shows \( p_0^n \le \frac{2}{\sqrt
{%
n+1}} \).  The next subsection has an example that \( p_0^n \) can be as
large as \( \frac{1}{\sqrt{2\pi n}} \).  The example means the \( O (\sqrt
{\epsilon}) \) probability from Corollary~%
\ref{thm:serialsignificance:sqrtepstest}, as the worst possible behavior
for \( p_0^n \), can actually be achieved.

In the general setting of a reversible Markov chain, the Corollary leads
to a simple quantitative procedure for asserting rigorously \( \sigma_0 \)
is atypical with respect to \( \pi \) without knowing the mixing time of
\( X \):  simply observe a random trajectory \( \sigma_0 = X_0 , X_1, X_2,
\dots, X_n \) from \( \sigma_0 \) for any fixed \( n \).  If \( v(\sigma_0
) \) is an \( \epsilon \)-outlier among \( v(X_0), v(X_1), \dots, v(X_n)
\), then the probability of \emph{erroneously} concluding \( X_0 \) is
not unusual, when in fact the null hypothesis that \( X_0 \) is not
unusual is true, is less than \( \sqrt{2 \epsilon } \).

Corollary~%
\ref{thm:serialsignificance:sqrtepstest} leads to the Outlier Test
Algorithm:%
\index{Outlier Test Algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \KwData{Markov chain \( X \), starting state \(
    \sigma_0 \), value function \( v(\cdot) \)}
  \KwResult{probability \(
    \sqrt{2 \epsilon} \) of \emph{erroneously} concluding \( X_0 \) is
    \emph{not unusual} with respect to the value function.} Begin from
  the state being evaluated for ``outlier'' status.\;
  Make a sequence of \( n \) random changes to the states according to the Markov
  chain transition probabilities.\;
  Evaluate the labeling of the chain.\;
  Call the original state an ``outlier'' if the overwhelming
  majority of values (more than \( 1 - \epsilon \) of the values) are
  greater in value than the starting state.\;
    \caption{Outlier Test Algorithm.}
\end{algorithm}

\begin{remark}
    The Outlier Test Algorithm is useful because \( \sqrt{2\epsilon} \)
    is reasonably close to \( 0 \) for \( \epsilon \) small; in
    particular, it is possible to obtain ``good enough'' statistical
    significance from observations which can be made with reasonable
    computational resources.  Of course, smaller probability compared to
    \( \epsilon \) would be even better, but, as already noted, \( p = O
    (\sqrt {\epsilon}) \) is the best possible with a single serial
    Markov chain.
\end{remark}

\begin{remark}
    The \( \sqrt{2\epsilon} \) test is a simple informal interpretation
    for the general behavior of reversible Markov chains.  Specifically,
    the long-term typical (i.e., stationary) states are unlikely (with \(
    p = O(\sqrt{\epsilon}) \)) to differ in a measurable way (be in the
    smallest \( \epsilon \) fraction measured by the value) under a
    sequence of chain transitions, with a best-possible quantification
    up to constant factors.
\end{remark}

Next is a corollary of Theorem~%
\ref{thm:serialsignificance:basethm} that applies to the setting where \(
X_0 \) is not distributed as the stationary distribution \( \pi \) but
instead is distributed with small total variation distance to \( \pi \).

\begin{corollary}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain on \( \mathcal{X} \).
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a value function.
        \item
            Let \( X_0 \) be a state chosen from \(
            \operatorname{dist}
            (\mathcal{X}) \) with distribution such that
            \[
                \|
                \operatorname{dist}
                (\mathcal{X}) - \pi \|_{TV} \le \epsilon_1.
            \]
    \end{enumerate}
    Then for any fixed \( n \), the probability the value of \( X_0 \)
    is an \( \epsilon \)-outlier from among the list of values observed
    in the trajectory \( X_0, X_1, \dots X_n \) is at most \( \sqrt{2\epsilon}
    + \epsilon_1 \)
\end{corollary}

\begin{proof}
    \begin{enumerate}
        \item
            If \( \rho_1 \), \( \rho_{2} \), and \( \rho_3 \) are
            probability distributions, then the product distributions \(
            (\rho_1 , \rho_3 ) \) and \( (\rho_2 , \rho_3 ) \) satisfy \(
            \| (\rho_1 , \rho_3 )- (\rho_2, \rho_3 )\|_{TV} = \|\rho_1-
            \rho_2 \|_{TV} \).% [Refer to a Theorem here?]
        \item
            Split the randomness in the trajectory \( X_0, \dots , X_n \)
            of the Markov chain into two independent sources:  the
            initial distribution is \( X_0 \sim
            \operatorname{dist}
            (\mathcal{X}) \), and \(
            \operatorname{U}
            \) is the uniform distribution on sequences of length \( n \)
            of real numbers \( r_1 , r_2, \dots, r_n \) in \( [0, 1] \).
        \item
            The distribution of the trajectory \( X_0 , X_1,\dots, X_n \)
            is the product \( (\rho,
            \operatorname{U}
            ) \) by using sequences of reals \( r_1, \dots, r_n \) to
            choose transitions in the chain; from \( X_i = \sigma_i \),
            if there are \( L \) transitions possible, with
            probabilities \( p_1, \dots, p_L \).
            %[Rewrite this and following steps for better clarity.]
        \item
            Then, make the \( t \)th possible transition if \( r_i \in [p_1
            + \cdots + p_{t-1}, p_1 + \cdots + p_{t-1 }+ p_t] \).  If \(
            \|
            \operatorname{dist}
            (\mathcal{X})- \pi\|_{TV} \le \epsilon_1 \), then \( \|(
            \operatorname{dist}
            (\mathcal{X}),
            \operatorname{U}
            )- (\pi,
            \operatorname{U}
            )\|_{TV} \le \epsilon_1 \).
        \item
            Therefore, any event that would happen with probability at
            most \( p \) for the sequence \( X_0 ,\dots, X_n \) when \(
            X_0 \sim \pi \) must happen with probability at most \( p +
            \epsilon_1 \) when \( X_0 \sim \rho \), where \( \|\rho- \pi\|_
            {TV} \le \epsilon_1 \).  The corollary follows.
    \end{enumerate}
\end{proof}

\subsection*{Simple Example of Unusual States}

\begin{example}
    Consider a specific simple case of the alternative Ehrenfest urn
    model.%
    \index{Ehrenfest urn model}
    The setup of the urn model is repeated here for convenient
    reference. Two urns, labeled \( A \) and \( B \), contain a total of
    \( N = 7 \) balls.  At each step a ball is selected at random with
    all selections equally likely.  Then an urn is selected, urn \( A \)
    with probability \( p = \frac{1}{2} \) and urn \( B \) with
    probability \( q = 1-p = \frac {1}{2} \) and the ball is moved to
    that urn.  The state at each step is the number of balls in the urn \(
    A \), from \( 0 \) to \( N \).  Start from state \( 0 \), with no
    balls in urn \( A \).  All states are accessible and all states
    communicate, so the chain is irreducible.  All states are recurrent
    and there are no transient states.  The chain is periodic with
    period \( 1 \) so it is regular.  The standard Ehrenfest urn model,
    without the random choice of urns at each step, is periodic with
    period \( 2 \).  Having period \( 1 \) makes interpretation of the
    results simpler.

    For this simple example, let the value function be the number of
    balls in urn \( A \), so that the value function is identical with
    the state number.  The simplicity of the alternative Ehrenfest urn
    model limits the possibilities for value functions.  More
    interesting value functions appear in the gerrymandering example
    later, where the states have considerably more structure and
    information associated with them.

    The second largest (in absolute magnitude) eigenvalue of the
    transition probability matrix is approximately \( 0.857 \).  The
    total variation distance of the probability distribution of the
    states at step \( n \) should differ from the stationary
    distribution by a multiple of \( (0.857)^n \).  Therefore, it takes
    about \( 30 \) steps for the total variation distance of the
    probability distribution of the states to differ from the stationary
    distribution by less than \( 0.01 \).  For the alternative Ehrenfest
    urn model with \( 7 \) balls, the stationary distribution of the
    Markov chain started from state \( 0 \) will be in state \( 0 \)
    about \( 0.78125\% \) of the steps.  In \( 199 \) steps, together
    with the starting state \( 0 \), it would be typical to see the
    state \( 0 \) just once or twice.  Equivalently, the mean recurrence
    time to state \( 0 \) is \( 128 \) steps, so again in \( 199 \)
    steps it would be typical to see the state \( 0 \) just once or
    twice.  This shows the state \( 0 \) is an unusual state with
    respect to the stationary distribution, which also should be
    expected from the statistical mechanical interpretation of the
    alternative Ehrenfest urn model.  The state \( 0 \) is a state of
    high statistical-mechanical order, with all the balls in urn \( B \),
    none in urn \( A \).  Such highly ordered states should be unlikely.
    In summary, the value \( 0 \) should be fairly unusual, in spite of \(
    0 \) being a state in the stationary distribution.

    If \( \epsilon = 0.01 \), then this analysis predicts that value \(
    0 \) would usually be an \( \epsilon \)-outlier for the set of
    values generated by the chain, since it would usually appear in the
    smallest \( \epsilon \)-fraction of the \( 200 \) ranked values,
    that is, the least two values.  Equivalently \( 198 \) of the \( 200
    \) values from the path would be greater than \( 0 \).  Actually
    running simulations of the alternative Ehrenfest Markov chain shows
    state \( 0 \) is an \( 0.01 \)-outlier according to this test about
    half of the time, see the exercises.  The simulations show that
    rejecting the null hypothesis, that is, concluding in error that \(
    0 \) is \emph{not unusual} with respect to the stationary
    distribution, occurs about half the time.

    However, with just \( 8 \) values, there is not much to distinguish
    being an \( \epsilon \)-outlier, and sampling variation among Markov
    chain runs could account for false negatives, that is, finding the
    truly unusual state \( 0 \) (unusual in the sense of high
    statistical-mechanical order and unusual in the sense of infrequent
    visitation) occurs in the larger \( 1 - \epsilon \) fraction of the
    values, asserting that value \( 0 \) is not unusual.  Experiments
    with more trials with longer path lengths for the Markov chain shows
    the proportion of trials showing \( 0 \) is an \( \epsilon \)-outlier,
    that is, the empirical probability, slowly increases.  Rejecting the
    null hypothesis increases with increasing the length of the chain,
    as expected.  This is an illustration of the problem of determining
    the statistical power of the test discussed later.

    Considering larger specific cases of the alternative Ehrenfest urn
    model%
    \index{Ehrenfest urn model}%
    with \( N = 21 \) and \( N = 51 \) balls illustrates the utility of
    a more discriminating value function.  With \( N = 21 \) balls,
    Markov chain runs of \( 399 \) steps in \( 400 \) trials the
    proportion showing \( 0 \) is an \( \epsilon \)-outlier is about \(
    0.92 \).  With longer Markov chain runs of \( 800 \) or \( 1600 \)
    steps in \( 1000 \) trials, the proportion showing \( 0 \) is an \(
    \epsilon \)-outlier is better than \( 0.97 \).  With \( N = 51 \)
    balls, the probability of showing state \( 0 \) is unusual in long
    runs is virtually certain.

    The important observation is that with a sufficiently long run, and
    a sufficiently discriminating value function, the \( \epsilon \)-outlier
    test can show state \( 0 \) is unusual with high confidence without
    knowing the mixing time, or that the Markov chain is in the steady
    state, or even the steady state distribution.

\end{example}

\subsection*{Example Where \( p = O(\sqrt{\epsilon}) \) Is the Best
Possible}

It might be natural to suspect that observing \( \epsilon \)-outlier
status for \( \sigma \) on a random trajectory from \( \sigma \) is
significant at \( p = O(\epsilon) \) instead of the probability \( p = O
(\sqrt{\epsilon}) \) established by Corollary~%
\ref{thm:serialsignificance:sqrtepstest}.  However, because Corollary~%
\ref{thm:serialsignificance:sqrtepstest} uses no information about the
mixing rate of \( X \) or the stationary distribution, it seems
remarkable any significance can be shown in general.  An example in this
section shows probability at \( p = O( \sqrt{\epsilon}) \) is the best
possible.

Given \( n \) and \( \epsilon \), choose \( N > \frac{2n+1}{\epsilon} \).
Let \( X \) be the Markov chain where \( X_0 \) is chosen at random
uniformly in \( \mathcal{X} = \set{0, 1, 2, \dots , N-1} \), and for any
\( \nu \ge 1 \), \( X_{\nu} \) is equal to \( X_{\nu-1} + \zeta_{\nu} \)
modulo \( N \), where \( \zeta_{\nu} = \pm 1 \) with equal probability \(
\frac{1}{2} \).  That is, \( X \) is random walk on the circle.  Note
the chain is reversible and has stationary distribution \( \pi_\nu =
\frac{1}{N} \).  % As a
% random walk, it would be unusual if the walk did not cross back and
% forth over the initial state \( X_0 \).  To measure if the walk is
% unusual,
The value function is the position around the circle, that is, \( v (X_{\nu})
= X_{\nu} \).  % Then an unusual walk, which does not
% cross back over \( X_0 \) would have \( v(X_0) \) as a minimum over the
% sampled walk.

Then \( N \) is large enough that the distance of \( X_0 \) from \( 0 \)
in both directions is greater than \( n \) with probability greater than
\( 1 - \epsilon \).  Conditioning on this event, \( X_0 \) is minimum,
i.e.\ among-1-smallest of \( X_0, \dots , X_n \) if and only if all of
the partial sums \( \sum\limits_{\nu=1}^{j} \zeta_\nu \) are positive
for \( j \le n \).  The probability of this event is just the
probability an \( n \)-step \( 1 \)-dimensional random walk from \( X_0 \)
takes a first step to the right and does not return to the origin.  The
calculation of this probability is a classical problem in random walks,
solved using the reflection principle.  In particular, for \( n \) even,
the probability is given exactly by (see the exercises)
\[
    \frac{1}{2^{n+1}}\binom{n}{n/2} \asympt \frac{1}{\sqrt{2 \pi n}}.
\] Because being the minimum or among-1-smallest of \( X_0, \dots , X_n \)
corresponds to being an \( \epsilon \)-outlier for \( \epsilon = \frac{1}
{n+1} \) this example shows the probability of being an \( \epsilon \)-outlier
can be as high as (a nearly unit multiple of) \( \sqrt{\epsilon/(2\pi)} \).
Of course, \( \sqrt{\epsilon/ (2\pi)} < \sqrt{2\epsilon} \) so the best
possible value of the constant in the \( \sqrt{2\epsilon} \) test is an
interesting question.

Figure~%
\ref{fig:serialsignificance:circlewalk} schematically illustrates a
specific example.  Let \( n = 198 \), \( \epsilon = 0.01 \), so \( N >
\frac{2n+1}{\epsilon} = 39,700 \).  Take \( N = 40{,}000 \) for
simplicity.  States, such as \( 20{,}000 \), chosen outside the region
marked with bars have the distance of \( X_0 \) from \( 0 \) in both
directions is greater than \( n = 198 \) with probability greater than \(
1 - \epsilon = 0.99 \).  An \( n \)-step random walk from \( X_0 \)
satisfying the condition is drawn schematically.

\begin{figure}
    \centering
\begin{asy}
        size(5inches);

        import geometry;

        real myfontsize = 12; real mylineskip = 1.2*myfontsize; pen
        mypen = fontsize(myfontsize, mylineskip); defaultpen(mypen);

        point O = (0,0); real r = 2; circle C = circle(O,r); draw(C);

        dot("\( 0 \)", relpoint(C, 0.25), N); dot("\( 1 \)", relpoint(C,
        0.24), N); dot("\( 2 \)", relpoint(C, 0.23), N);

        dot(relpoint(C, 0.21)); dot(relpoint(C, 0.20)); dot(relpoint(C,
        0.19));

        dot("\( 199 \)", relpoint(C, 0.15), NE);

        dot("\( 39{,}999 \)", relpoint(C, 0.26), S); dot("\( 39{,}998 \)",
        relpoint(C, 0.27), NW);

        dot(relpoint(C, 0.29)); dot(relpoint(C, 0.30)); dot(relpoint(C,
        0.31));

        dot("\( 39{,}801 \)", relpoint(C, 0.35), NW);

        arc avoided = arc(C, 51, 129); draw(avoided, Bars);

        dot("\( 20{,}000 \)", relpoint(C, -0.25), S); dot("\( 19{,}999 \)",
        relpoint(C, -0.24), NE); dot("\( 20{,}001 \)", relpoint(C, -0.26),
        NW);

        dot(relpoint(C, 0.00)); dot(relpoint(C, -0.01)); dot(relpoint(C,
        0.01));

        dot(relpoint(C, 0.50)); dot(relpoint(C, 0.49)); dot(relpoint(C,
        0.51));

        circle C1 = circle(O, 0.92*r); circle C2 = circle(O, 0.91*r);
        circle C3 = circle(O, 0.90*r);

        arc walk1 = arc(C1, -90, -130, direction=CW); arc walk2 = arc(C2,
        -130, -110, direction=CCW); arc walk3 = arc(C3, -110, -140,
        direction=CW);

        draw(walk1, Arrow); draw(walk2, Arrow); draw(walk3, Arrow);
\end{asy}
    \caption{A schematic diagram of the random walk on a circle
    illustrating the example of the best possible bound.}%
    \label{fig:serialsignificance:circlewalk}
\end{figure}
\subsection*{Notes on Statistical Power}

The effectiveness of the \( \sqrt{2\epsilon} \) test depends on the
availability of a sufficiently discriminating choice for the value
function \( v \) and the ability to run the test for long enough (in
other words, choose \( n \) large enough) to detect the presented state
is an \( \epsilon \)-outlier.  It is possible, however, to make a
general statement about the power of the test when \( n \) is chosen
large relative to the actual mixing time of the chain.  Recall one
potential application of the test is in situations where the mixing time
of the chain is actually accessible through reasonable computational
resources, although this fact cannot be proved rigorously, because
theoretical bounds on the mixing time are usually not available.  In
particular, the test is very likely to succeed when \( n \) is
sufficiently large and \( v(\sigma_0) \) is unusual as already shown by
the alternative Ehrenfest urn model.

\begin{remark}
    Let \( \tau_{\text{rel}} \) be the \emph{relaxation time}%
    \index{relaxation time}
    for \( X \) defined as \( 1/ (1-\lambda_*) \) where
    \[
        \lambda_* = \max \setof{\abs{\lambda}}{ \lambda \text{ is an
        eigenvalue of } P, \lambda \ne 1 }
    \] is the \emph{absolute spectral gap} for \( X \).  The relaxation
    time is related to the mixing time of \( X \).  Let \( d(t) = \sup_{\sigma}
    \| P^t - \pi \|_{TV} \).  The \emph{mixing time} is \( \tau_{\text{mix}}
    (\epsilon) = \min\setof{t}{d(t) \le \epsilon} \).  For reversible
    chains~%
    \cite[Theorems 12.3, 12.4]{levin09},
    \[
        (\tau_{\text{rel}}-1) \log \left( \frac{1}{2\epsilon} \right)
        \le \tau_{\text{mix}}(\epsilon) \le \tau_{\text{rel}} \log
        \left(\frac{1} {\epsilon \min_x \pi(x)} \right).
    \] In some cases the relaxation time can be calculated from the
    eigenvalues of the transition probability matrix to give bounds on
    the mixing time.  Typically, the mixing time is difficult to find
    directly.
\end{remark}

Theorem~%
\ref{thm:serialsignificance:powerthm} below is a simple application of
the following theorem of Gillman~%
\cite[Theorem 2.1]{gillman98} translated into the current notation and
assumptions.

\begin{theorem}[Gillman's Chernoff-Type Bound]
    %
    \label{thm:serialsignificance:gillman}
    \begin{enumerate}
        \item
            Let \( X = X_0, X_1, \dots \) be a reversible Markov chain
            on \( \mathcal{X} \).
        \item
            Let \( A \subset \mathcal{X} \).
        \item
            Let \( N_n(A) \) denote the number of visits to \( A \)
            among \( X = X_0, X_1, \dots, X_n \).
    \end{enumerate}
    Then for any \( \gamma > 0 \),
    \[
        \Prob{N_n(A)/n - \pi(A) \ge \gamma} \le \left( 1 + \frac{\gamma}
        {10 \tau_{\text{rel}}} \right) \sqrt{\sum_{\sigma} \frac{(\Prob{X_0
        = \sigma})^2}{\pi (\sigma)}} \exp \left( \frac{-\gamma^2 \cdot n}
        {20 \tau_{\text{rel}}} \right).
    \]
\end{theorem}
\index{Gillman's Chernoff-Type Bound}

\begin{theorem}
    \label{thm:serialsignificance:powerthm}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain on \( \mathcal{X} \).
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a value function.
        \item
            Given \( \sigma_0 \), suppose for a random state \( \sigma
            \sim \pi \),
            \[
                \Prob{v(\sigma) \le v(\sigma_0)} \le \epsilon.
            \]
    \end{enumerate}
    Then with probability \( p \), where
    \[
        p \ge 1 - \left( 1 + \frac{\epsilon}{10 \tau_{\text{rel}}}
        \right) \cdot \frac{1}{\sqrt{\pi_{\min}}} \cdot \exp \left(
        \frac {-\epsilon^ {2} \cdot n}{20 \tau_{\text{rel}}} \right),
    \] \( v(\sigma) \) is a \( 2\epsilon \)-outlier among \( v(\sigma_0),
    v(\sigma_1), \dots, v(\sigma_n) \) where \( \sigma_0, \sigma_1,
    \dots, \sigma_n \) is a random trajectory starting from \( \sigma_0 \).
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item
            Apply Theorem~%
            \ref{thm:serialsignificance:gillman} with \( A \) as the set
            of states \( \sigma \in \mathcal{X} \) such that \( v(\sigma)
            \le v(\sigma_0) \), \( X_0 = \sigma_0 \) and \( \gamma =
            \epsilon \).  By assumption \( \pi(A) \le \epsilon \).
        \item
            Theorem~%
            \ref{thm:serialsignificance:gillman} gives
            \[
                \Prob{N_{n}(A)/n > 2\epsilon} \le \left( 1 + \frac{\epsilon
                n}{10 \tau_{\text{rel}}} \right) \cdot \frac{1}{\sqrt{\pi_
                {\min}}} \cdot \exp \left( \frac{-\epsilon^{2 n}}{20
                \tau_{\text{rel}}} \right).
            \]
        \item
            The proof uses the additional simplification:
            \[
                \frac{(\Prob{X_0 = \sigma})^2}{\pi (\sigma)} \le \frac{(\Prob
                {X_0 = \sigma})^2}{\pi_{\min}}
            \] so
            \[
                \sum_{\sigma} \frac{(\Prob{X_0 =\sigma})^2}{\pi (\sigma)}
                \le \frac{\sum_{\sigma}{(\Prob{X_0 =\sigma})^2}}{\pi_{\min}}
                \le \frac{(\sum_{\sigma}\Prob{X_0 =\sigma})^2}{\pi_{\min}}
                = \frac{1}{\pi_{\min}}
            \] using standard inequalities.
    \end{enumerate}
\end{proof}

\begin{remark}
    The probability \( p \) in Theorem~%
    \ref{thm:serialsignificance:powerthm} converges in \( n \)
    exponentially quickly to \( 1 \).  The probability \( p \) is very
    close to \( 1 \) if \( n \) is large relative to \( \tau_{\text {rel}}
    \).  In particular, Theorem~%
    \ref{thm:serialsignificance:powerthm} shows the \( \sqrt{2\epsilon} \)
    test will work when the test runs long enough.  Of course, one
    strength of the \( \sqrt{2\epsilon} \) test is that it can sometimes
    detect unusual states, even when \( n \) is far too small to mix the
    chain, which is almost certainly the case for the application to
    gerrymandering later.  When these short-\( n \) runs are successful
    at detecting unusual states is, of course, dependent on the
    relationship of the presented state \( \sigma_0 \) and its local
    neighborhood in the chain.  An example is the random walk on the
    circle.
\end{remark}

\subsection*{Application to Political Districting}

Pennsylvania has \( 18 \) Congressional districts.  According to the
``one person, one vote'' principle, Congressional districts are required
by law to be roughly equal in population. Pennsylvania is divided into
roughly \( 9,000 \) voting precincts.  Define a division of these
precincts into \( 18 \) districts to be a \emph{valid districting} of
Pennsylvania if each district differs in population from uniform
equality by less than \( 2\% \), are contiguous, are simply connected (districts
do not contain holes), and are ``compact''.  Roughly, this last
condition prohibits districts with extremely contorted structure.  The
state space of the Markov chain is the set of valid districtings of
Pennsylvania. One step of the Markov chain consists of randomly swapping
a precinct on the boundary of a district to a neighboring district if
the result is still a valid districting.  The chain has a technical
modification to ensure the uniform distribution on valid districting is
indeed the stationary distribution for the chain, see~%
\cite{chikina2860si} for details. Observe that this Markov chain has a
potentially huge state space; if the only constraint on valid
districting was that the districts have roughly equal population, there
would be \( 10^{10000} \) or so valid districtings.  (See the exercises
for a justification of this estimate.) Although contiguity and
especially, compactness are severe restrictions that will decrease this
number substantially, it seems difficult to compute effective upper
bounds on the number of resulting valid districtings, and certainly, the
number of valid districtings is still enormous.  Impressively, these
considerations are all immaterial to the general method of significance
testing.  Regarding mixing time, even in chains with relatively weak
constraints on the districtings (and fast running time in implementation),
the chains seem to mix too slowly to sample \( \pi \), even
heuristically.

Applying the \( \sqrt{2\epsilon} \) test involves the choice of a value
function \( v(\sigma) \), which assigns a real number to each
districting.  Two useful value functions are
\begin{itemize}
    \item
        \( v_{\text{var}} \) is the (negative) variance of the
        proportion of Democrats in each district of the districting (as
        measured by quadrennial presidential votes), and
    \item
        \( v_{\text{MM}} \) is the difference between the median and
        mean of the proportions of Democrats in each district.
\end{itemize}
The variance can change quickly with small changes in districtings,
making it a sensitive value function.  The median-mean difference \( v_{\text
{MM}} \) metric has a long history in gerrymandering analysis.  An
important point is that these value functions are not based on an
assumption that small values of \( v_{\text {var}} \) or \( v_{\text{MM}}
\) directly imply gerrymandering.  Instead, because the \( \sqrt{2\epsilon}
\) test is valid for any fixed value function, these values are tools
used to show statistical significance.  Use these value functions
because they are simple and natural functions on vectors that can be
quickly computed, seem likely to be different for typical versus
gerrymandered districtings, and have the potential to change relatively
quickly with small changes in districtings.  For chains of length \( 2^{40}
\) steps with various notions of valid districtings differing in
compactness measure and threshold, the \( \sqrt{2\epsilon} \) test
showed significance in the range from \( 10^{-4} \) to \( 10^ {-5} \)
for the \( v_{\text {MM}} \) value function and the range from \( 10^{-4}
\) to \( 10^{-7} \) for the \( v_{\text{var}} \) value function,
see~\cite{chikina2860si}.

It should also be kept in mind that, although the \( \sqrt{2\epsilon} \)
Test gives a method to show the districting is unusual among
districtings distributed according to the stationary distribution \( \pi
\), it does so without giving a method to sample from the stationary
distribution.  In particular, the method cannot answer the question of
how many seats Republicans and Democrats should have in a \emph{typical}
districting of Pennsylvania, because the chain is not selecting from the
stationary distribution.  Instead, the \( \sqrt{2\epsilon} \) gives a
way to \emph{disprove} the null hypothesis that a given districting is
\emph{not unusual} without sampling \( \pi \).  Consider the following
specialization of the Local Outlier Algorithm to test the political
districting of a state:\\
\index{Local Outlier Algorithm}
\begin{enumerate}
    \item
        Begin with the districting being evaluated.
    \item
        \label{enum:serialsignificance:mcstep} Make a sequence of random
        changes to the districting, while preserving some set of
        constraints imposed on the districtings.
    \item
        Evaluate the partisan properties of each districting encountered
        by simulating elections using past voting data and measuring \(
        v_{\text {var}} \) or \( v_{\text{MM}} \).
    \item
        \label{enum:serialsignificance:crafted} Call the original
        districting ``carefully crafted'' or ``gerrymandered'' if the
        overwhelming majority of districtings produced by making small
        random changes are less partisan than the original districting.
\end{enumerate}

Naturally, the test described above can be implemented so it precisely
satisfies the hypotheses of the Theorems in this section.  For this
purpose, a (very large) set of comparison districtings are defined, to
which the districting being evaluated belongs.  For example, the
comparison districtings may be the districtings built out of precincts (or
some other unit) which are contiguous, equal in population up to some
specified deviation, or include other constraints.  A Markov chain \( X \)
randomly walks through the districtings, where transitions in the chain
correspond to changes in districtings.  For example, a transition may
correspond to randomly changing the district assignment of a randomly
chosen Census Block which currently borders more than one district,
subject to the constraints imposed on the comparison set.  This is
called a ``flip chain'' on the state space of all feasible districting.
Alternatively, a ``ReCom chain'' method works as follows:  merge two
districts in the plan, generate a minimum spanning tree for the
precincts in the merged district, then ``split'' the merged district
into two new districts by finding a population-balanced cut of the
minimum spanning tree.  The ``random changes' from Step~%
\ref{enum:serialsignificance:mcstep} will then be precisely governed by
the transition probabilities of the Markov chain \( X \).  By designing \(
X \) so the uniform distribution \( \pi \) on the set of possible
districtings \( \mathcal{X} \) is the stationary distribution for \( X \),
Theorem~%
\ref{thm:serialsignificance:sqrtepstest} gives an upper bound on the
false positive, i.e.\ Type I error, probability for the null hypothesis
of ``not gerrymandered'' declaration made in Step~%
\ref{enum:serialsignificance:crafted}.

\visual{Section Starter Question}{../../../../CommonInformation/Lessons/question_mark.png}
\section*{Section Ending Answer}

Making a Type I error would be to do the statistical test, and \emph{reject}
the null hypothesis that the state is ``not unusual'' with respect to
some function valuing the states when in fact, the state \emph{is}
typical with respect to the valuing.

Making a Type II error would be to do the statistical test, and \emph{not
reject} the null hypothesis that the state is ``not unusual'' with
respect to some function valuing the states when in fact, the state
\emph{is} unusual with respect to the valuing.  That is, to conclude
that the state is typical when in fact it is unusual.

\subsection*{Sources} This section is adapted from~%
\cite{doi:10.1080/2330443X.2020.1806763, Chikina2860, Chikina2019}.

The first paragraph of the remark following Proposition~%
\ref{thm:serialsignificance:basethm} about Markov chains with distinct
irreducible components is adapted from
https://math.stackexchange.com/questions/1636992/an-example-of-a-reversible-but-reducible-markov-chain

The statement of Gillman's Chernoff-Type Bound is modified from~%
\cite[Theorem 2.1]{gillman98}.

The remark about relaxation times and mixing times is adapted from~%
\cite{levin09} and
https://math.stackexchange.com/questions/1382222/relaxation-time-and-mixing-time-of-markov-chains

\hr

\visual{Algorithms, Scripts, Simulations}{../../../../CommonInformation/Lessons/computer.png}
\section*{Algorithms, Scripts, Simulations}

\subsection*{Algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \SetKwData{pL}{pathLength}
  \SetKwData{nT}{nTrials}
  \SetKwData{res}{results}
  \SetKwData{oT}{outlierThreshold}
  \SetKwData{nEO}{nEpsilonOutlier}
  \SetKwData{pEO}{probEpsOutlier}
  \SetKwFunction{Floor}{floor}

  \Input{Number of balls \( N \) and probability of urn switch, \( p \)}
  \Output{Table of empirical probabilities of \( \epsilon \)-Outlier
    Test success.}
  \BlankLine
  \( N \leftarrow 7 \), \( p \leftarrow 1/2 \),
  \( q \leftarrow 1-p \)\;
  \tcp{Build \( (N+1) \times (N+1) \) transition probability matrix}
  \( P_{11} \leftarrow q \), \( P_{12} \leftarrow p \)\;
  \( P_{N+1,N} \leftarrow q \), \( P_{N+1, N+1} \leftarrow p \)\;
  \For{\( r \leftarrow 2 \) \KwTo \( N \)}{
    \( P_{r,r-1} \leftarrow (r-1)/N \cdot q \)\;
    \( P_{r,r} \leftarrow (N-(r-1))/N \cdot q + (r-1)/N \cdot p \)\;
    \( P_{r,r-1} \leftarrow (N-(r-1))/N \cdot p \)\;
  }
  \tcp{initialize and build Markov chain}
  \( X_0 \leftarrow 0 \)\;
  Build Markov chain object\;
  \BlankLine
  Set \pL array, \nT array, \( \epsilon \), initialize \res\;
  \ForEach{element in \pL}{
    \ForEach{element in \nT} {
      \oT \(\leftarrow \) \Floor( (\( \epsilon \cdot \) \pL +1) )+1\;
      Initialize \nEO \( \leftarrow 0 \)\;

      \ForEach{\( i \leftarrow 1 \) \KwTo \nT }{
        Run Markov chain of length \pL\;
        Prepend initial state \( X_0 \)\;
        \If{ \( X_0 < \) \oT } {
          increment \nEO}
      }
      \pEO \( \leftarrow \) \nEO\(/\)\nT\;
      Update results table with \pEO;
    }
  }
  Report results table\;
\end{algorithm}
\subsection*{Scripts}

\input{serialsignificance_scripts}

\hr

\visual{Problems to Work}{../../../../CommonInformation/Lessons/solveproblems.png}
\section*{Problems to Work for Understanding}
\renewcommand{\theexerciseseries}{}
\renewcommand{\theexercise}{\arabic{exercise}}

\begin{exercise}
    Compare the notion of \( \epsilon \)-outlier of a set of \( n+1 \)
    real numbers \( \set{\alpha_0, \alpha_1, \dots \alpha_n} \) to the
    median of the set.
\end{exercise}
\begin{solution}
    If \( n \) is odd, the median \( \mu_{1/2} \) among \( \alpha_0,
    \alpha_1, \dots \alpha_n \) (with repetitions possible in the even
    number \( n+1 \) of values) has \( \frac {n+1}{2} \) indices with
    \( \alpha_i \le \mu_{1/2} \).  The median may not be a value
    in the set.  To say that \( \alpha_0 \) is a \( 1/2 \)-outlier means
    the number of indices of values at most as large as \( \alpha_0 \)
    is at most \( (n+1)/2 \), in other words \( \card{%
    \setof{\nu}{\alpha_{\nu} \le \alpha_0}} \le \frac{1}{2}(n+1) \).
    Then \( \alpha_0 \) is at most as large as the median, \( \alpha_0
    \le \mu_{1/2} \).  Also \( \alpha_0 \) is a member of the set while
    the median may not be.  The median focuses more on the middle among
    the values, splitting the values into halves, ranked by value.
    However ``\( \alpha_0 \) is a \( 1/2 \)-outlier'' focuses more on
    whether the count of indices of values at most as large as \( \alpha_0
    \) is at most half of all indices, so that \( \alpha_0 \) is among
    the half of smaller values ranked by order.

    If \( n \) is even, so \( n + 1 \) is odd, the median is a member of
    the list.  If \( n \) is even, a real number \( \mu_{1/2} \) is a
    median among \( \alpha_0, \alpha_1, \dots \alpha_n \) (with
    repetitions possible) if there are \( \frac{n}{2} \) indices for
    which \( \alpha_i \le \mu_ {1/2} \).  So if \( n \) is even and if \(
    \alpha_0 \) happens to be the median, then \( \alpha_0 \) is not a \(
    \frac{1}{2} \)-outlier because there are least \( n/2 + 1 \)
    values at most \( \alpha_0 \), including \( \alpha_0 \) itself,
    possibly more if there are repetitions of \( \alpha_0 \) in the set.  That is, if \(
    \alpha_0 \) is the median, it is not among at most \( (n+1)/2 = n/2
    + 1/2 \) (really just \( n/2 \)) indices.  However, if \( \alpha_0 \)
    is strictly less than the median \( \mu_{1/2} \), \( \alpha_0 < \mu_{1/2}
    \), then \( \alpha_0 \) is a \( 1/2 \)-outlier because the count of
    indices at most as large as \( \alpha_0 \) is at most \( n/2 \)
    which is less than \( (n+1)/2 \).

    The point is that considering values only, as with the median,
    repetitions or equality among values do not matter.  However, with
    the \( 1/2 \)-outlier, repetitions or equality do matter.  The point
    is whether the value \( \alpha_0 \) appears infrequently, i.e among
    the \( \epsilon \)-fraction of smallest ordered values. The
    comparisons are similar for the first quartile and a \( 1/4 \)-outlier
    and the first percentile and a \( 0.01 \)-outlier.
\end{solution}

\begin{exercise}
    For a set of real values \( \set{\alpha_0, \dots \alpha_n} \),
    compare the definition of ``\( \epsilon \)-outlier'' and ``among-\(
    \ell \)-smallest'' to the notions of the rank of an element in the
    set and the rank of elements of a set.
\end{exercise}
\begin{solution}
    For a set with no duplicates (or ties), \( \alpha_0 \) is an \(
    \epsilon \)-outlier of the set if and only if the rank (by
    increasing value) of \( \alpha_0 \) in \( \set{\alpha_0, \dots
    \alpha_n} \) is less than or equal to \( \lfloor \epsilon(n+1)
    \rfloor \).

    For a set with no duplicates (or ties), \( \alpha_0 \) is among-\(
    \ell \)-smallest if and only its order (increasing) in \( \set{\alpha_0,
    \dots \alpha_n} \) is at most \( \ell \).

    For a set with duplicates (or ties), the rank is not well-defined
    since there are several methods of ranking the ties.  For example,
    for the R function \texttt{rank()}, the user can specify the
    argument \texttt{ties.method} to determine the result at the
    corresponding indices.  The \texttt{first} method results in a
    permutation with increasing values at each index set of ties, and
    analogously \texttt{last} with decreasing values.  The \texttt{random}
    method puts these in random order whereas the default, \texttt{average},
    replaces them by their mean, and \texttt{max} and \texttt{min}
    replaces them by their maximum and minimum respectively, the latter
    being the typical sports ranking.  See the examples in the R
    documentation for \texttt{rank()}.

    For example, consider the set of \( 11 \) values
    \begin{multline*}
        \alpha_0 = 3, \alpha_1 = 1, \alpha_2 = 4, \alpha_3 = 1, \alpha_4
        = 5, \alpha_5 = 9, \\
        \alpha_6 = 2, \alpha_7 = 6, \alpha_8 = 5, \alpha_9 = 3, \alpha_
        {10} = 5
    \end{multline*}
    and ask for the value with rank \( 1 \).  With the \( \texttt{first}
    \) method the rank \( 1 \) element is \( \alpha_1 = 1 \).  With the \(
    \texttt{last} \) method the rank \( 1 \) element is \( \alpha_3 = 1 \).
    With the \( \texttt{random} \) method the rank \( 1 \) element may
    be \( \alpha_1 = 1 \) or \( \alpha_3 = 1 \).  With the \( \texttt{average}
    \) method the rank \( 1 \) element has value \( 1 \).  With the
    \texttt{max} and \texttt{min} methods the rank \( 1 \) elements are \(
    \alpha_3 = 1 \) or \( \alpha_1 = 1 \) respectively.

    However, the\( \epsilon \)-outlier is always well-defined with \(
    \epsilon = 0.1 \).  Then \( \alpha_0 = 3 \) is \emph{not} an \( 0.1 \)-outlier
    since \( 4 = \card{%
    \setof{\nu}{\alpha_{\nu} \le \alpha_0}} \ge \epsilon(n+1) = 1.1 \).
    In fact, \( \alpha_0 = 1 \) \emph{is not} a \( 0.2 \)-outlier or
    even a \( 0.3 \)-outlier.

    The point is that with rank and order, as with \( \epsilon \)-outlier,
    repetitions or equality among values matter a lot.  The point with \(
    \epsilon \)-outlier is whether the value \( \alpha_0 \) appears
    infrequently, i.e.\ only among the \( \epsilon \)-fraction of
    smallest ordered values, but not how to assign its place in the
    ordered values especially when repetitions occur.
\end{solution}

\begin{exercise}
    Using Table~%
    \ref{tab:serialsignificance:recordpaths}, explain how the values \(
    [5,3] \), \( [10, 7] \), and \( [n, 11] \) in  Table~%
    \ref{tab:serialsignificance:rhon}
    are obtained.
\end{exercise}
\begin{solution}
    \begin{enumerate}
        \item
            The entry \( [5,3] = 5 \) records the \( 5 \) trials where
            the value of \( X_5 \), is among-3-smallest.  That is, the
            number of trials where the number of values in a sample path
            less than or equal to \( X_5 \) is less
            than or equal to \( 3 \).  These would be trials 3, 4, 6,
            13, and 15.
        \item
            The entry \( [10, 7] = 5 \) records the \( 5 \) trials
            where the value of \( X_{10} \) is among-\( 7 \)-smallest.
            That is, the number of trials where the number of values
            in a sample path is less than or equal to \(
            X_{10} \), is less than or equal to \( 7 \).  In other words,
            trials where 7 values in the path are less than or
            equal to than the value of \( X_{10} \).  These would be
            trials 1, 5, 9, 11, and 15.
        \item
            The entries \( [n, 11] = 15 \) record the number of trials
            where the value of \( X_{10} \) is among-\( 11 \)-smallest.
            That is, the number of trials where the number of values
            less than or equal to \(   X_{10} \), is less than or equal
            \( 11 \), which is all the
            path.  This is true for all paths, so the
            entries are all \( 15 \).
    \end{enumerate}
\end{solution}

\begin{exercise}
    Given a Markov chain \( X \) with value function \( v:  \mathcal{X}
    \to \Reals \) and stationary distribution \( \pi \), define for each
    \( j, \ell \le n \) a real number \( \rho(j; \ell, n) \) which is
    the probability that for a \( \pi \)-stationary trajectory \( X_1, X_2,
    \dots X_n \) the value \( v(X_j) \) is among-\( \ell \)-smallest in \(
    v(X_1), v(X_2), \dots v(X_n) \).  Show \( \rho(j; \ell, n) \) is
    well-defined.
\end{exercise}
\begin{solution}
    The probability of a specific \( \pi \)-stationary trajectory \( X_1, X_2,
    \dots X_n \) can be calculated as usual.  This determines the probability of
    the value sequence \( v(X_1), v(X_2), \dots v(X_n) \).
    The event that the value \( v(X_j) \) is among-\( \ell \)-smallest
    in \( v(X_1), v(X_2), \dots v(X_n) \) is a subevent of the event \(
    v(X_1), v(X_2), \dots v(X_n) \).  Then \( \rho(j; \ell, n) \) which
    is the probability that for a \( \pi \)-stationary trajectory \( X_1,
    X_2, \dots X_n \) the value \( v(X_j) \) is among-\( \ell \)-smallest
    in \( v(X_1), v(X_2), \dots v(X_n) \) is the probability of this
    subevent  So ultimately, the transition probability matrix is the
    only data determining \( \rho(j; \ell, n) \).
\end{solution}

\begin{exercise}
    Use the script to find the proportion of trials for which the state \(
    0 \) is an \( 0.01 \)-outlier for the alternative Ehrenfest urn
    model with \( N = 7, 21, 51 \) balls and path length of the chain \(
    199, 399, 599, 999 \) steps when executing \( 200, 400, 800 \) and \(
    1600 \) trials.  Display the results in three charts, one for each \(
    N \).
\end{exercise}
\begin{solution}

\begin{lstlisting}[language=R]
        library(markovchain)

        N <- 51; p <- 1/2; q <- 1 - p; stateNames <- as.character( 0:N )
        ## Be careful here, because states numbered from 0, ## but R
        indexes from 1 transMatrix <- matrix(0, N+1,N+1) transMatrix[1,1]
        <- q transMatrix[1,2] <- p transMatrix[N+1, N ] <- q transMatrix
        [N+1, N+1] <- p for (row in 2:N) { transMatrix[row, row-1] <- ((row-1)/N)*q
        transMatrix[row,row] <- ((N-(row-1))/N)*q + ((row-1)/N)*p
        transMatrix[row,row+1] <- ((N-(row-1))/N)*p }

        startState <- "0"

        altEhernfest <- new("markovchain", transitionMatrix=transMatrix,
        states=stateNames, name="AltEhernfest")

        pathLength <- c(199, 399, 599, 999) nTrials <- c(200, 400, 800,
        1600)

        eps <- 0.01

        results <- matrix(0, length(pathLength), length(nTrials))

        for (pL in 1:length(pathLength)) { for (nT in 1:length(nTrials))
        {%

        outlierThreshold <- floor( eps * (pathLength[pL] + 1) ) + 1

        nEpsOutlier<- 0

        for (i in 1:nTrials[nT]) {%

        path <- rmarkovchain(n=pathLength[pL], object = altEhernfest, t0
        = startState) fullPath <- c(startState, path) labelFullPath <-
        as.numeric(fullPath) labelStartState <- as.numeric(startState)
        if (!identical(sort(labelFullPath)[outlierThreshold],
        labelStartState)) { nEpsOutlier <- nEpsOutlier + 1 } }

        probEpsOutlier <- nEpsOutlier/nTrials[nT]

        results[pL, nT] <- probEpsOutlier } }

        results
\end{lstlisting}

    \begin{enumerate}
        \item
            \( N= 7 \)

\begin{verbatim}
        [,1]   [,2]    [,3]     [,4]
[1,] 0.430 0.3975 0.38875 0.407500
[2,] 0.475 0.4800 0.48125 0.488125
[3,] 0.555 0.5675 0.52500 0.530000
[4,] 0.545 0.5900 0.59750 0.577500
\end{verbatim}
        \item

            \( N = 21 \)

\begin{verbatim}
      [,1]   [,2]    [,3]     [,4]
[1,] 0.730 0.7250 0.75250 0.740625
[2,] 0.875 0.9200 0.92625 0.920000
[3,] 0.985 0.9825 0.98625 0.975000
[4,] 1.000 1.0000 1.00000 0.999375
\end{verbatim}

        \item
            \( N = 51 \)

\begin{verbatim}
      [,1]   [,2]    [,3]     [,4]
[1,] 0.775 0.7425 0.73500 0.734375
[2,] 0.930 0.9275 0.94125 0.935625
[3,] 0.980 0.9850 0.98375 0.979375
[4,] 0.995 0.9975 0.99875 1.000000
\end{verbatim}

    \end{enumerate}

    These experiments show the proportion of trials for which the state \(
    0 \) is an \( 0.01 \)-outlier for the alternative Ehrenfest urn
    model is roughly constant with respect to the length of the chain,
    but slowly increases with the number of trials.  The experiments
    also show the proportion of trials for which the state \( 0 \) is an
    \( 0.01 \)-outlier for the alternative Ehrenfest urn model increases
    with the number of values allowed by the value function, or
    equivalently the sensitivity of the value function.
\end{solution}

\begin{exercise}
    Consider the bathroom occupancy example in the section on Stationary
    Distributions.  That Markov chain is irreducible and aperiodic.
    Because the chain is formulated as a random walk on a weighted
    graph, it is reversible.  Let the states be valued as ``OOO''\( =1 \),
    ``OOV''\( =2 \), ``OFO''\( =3 \), ``OCO''\( =4 \), ``VO''\( =5 \),
    ``VV''\( =6 \), so the greater value is a more desirable state.
    Using \( \epsilon=0.1 \) and a path length of \( 1000 \) steps
    starting from ``OOO'', is the least desirable state ``OOO'' unusual
    with respect to this value function in the sense of being an \(
    \epsilon \)-outlier?

    The transition probability matrix is
    \[
        P= \bordermatrix{       & OCO   & OFO   & OOO   & OOV   & VO
        & VV \cr
        OCO     & 1/2   & 0     & 0     & 0     & 0     & 1/2 \cr
        OFO     & 0     & 1/2   & 0     & 0     & 1/4   & 1/4 \cr
        OOO     & 0     & 0     & 1/2   & 0     & 1/2   & 0 \cr
        OOV     & 0     & 0     & 0     & 1/2   & 1/2   & 0 \cr
        VO      & 1/6   & 1/6   & 1/6   & 0     & 1/2   & 0 \cr
        VV      & 1/6   & 1/6   & 0     & 1/6   & 0     & 1/2 \cr
        }.
    \]
\end{exercise}

\begin{solution}
    Using this R script, the answer is \emph{yes}.  This says starting
    from ``OOO'', the probability of seeing such an unusual state with
    respect to this value function is less than \( 0.1 \).  While this
    is not a terribly small probability, in the context of the bathroom
    example, it is somewhat reassuring!

\begin{lstlisting}[language=R]
        library("markovchain")

        rows = 6

        stateNames <- c("OCO","OFO","OOO", "OOV", "VO", "VV") P <-
        matrix(c(1/2, 0, 0, 0, 0, 1/2, 0, 1/2, 0, 0, 1/4, 1/4, 0, 0,
        1/2, 0, 1/2, 0, 0, 0, 0, 1/2, 1/2, 0, 1/6, 1/6, 1/6, 0, 1/2, 0,
        1/6, 1/6, 0, 1/6, 0, 1/2), nrow=rows, byrow=TRUE) rownames(P) <-
        stateNames colnames(P) <- stateNames

        startState <- "OOO"

        bathroom <- new("markovchain", transitionMatrix=P,
        states=stateNames, name="Bathroom_Occupancy")

        pathLength <- 1000

        eps <- 0.1 outlierThreshold <- floor( eps * (pathLength + 1) ) +
        1

        v <- function (x) { ## value the states, greater value is more
        desirable state switch(x,
        "OOO"=1,"OOV"=2,"OFO"=3,"OCO"=4,"VO"=5,"VV"=6) }

        bathroomHistory <- rmarkovchain(n=pathLength, object=bathroom,
        t0=startState)

        values <- rep(0, pathLength)

        for (i in 1:pathLength) { values[i] <- v(bathroomHistory[i]) }

        if (!identical(sort(values)[outlierThreshold], v(startState))) {%
        print(cat( startState, "IS an", eps, "outlier for the bathroom
        states\n")) } else { print(cat( startState, "is NOT an", eps,
        "outlier for the bathroom states\n")) }
\end{lstlisting}

    This should not be too surprising, since the probability of ``OOO''
    occurring in the stationary distribution is \( 1/12 \), so in \(
    n=1000 \) steps in the stationary distribution, one could expect to
    see ``OOO'' about \( 80 \) times, near the threshold of \( \epsilon
    \cdot n = 100 \).  The second largest eigenvalue of \( P \) is \(
    1/2 \), so expect the Markov chain to have a total variation
    distance from stationary about \( 0.01 \) in about \( 7 \) steps or
    so.
\end{solution}

\begin{exercise}
    Pennsylvania is divided into roughly \( 9{,}000 \) precincts.
    Define a division of these precincts into \( 18 \) congressional districts
    to be a valid districting of Pennsylvania if districts are equal in
    population.  For purposes of the problem, assume precincts
    are approximately equal in population.  (This is not a valid assumption,
    precincts can have widely varying populations between
    rural and urban areas.) Also assume the only constraint on a valid
    districting is the districts have roughly equal population.  (This
    is not the only constraint, districts are also required to be
    connected and to be compact, that is, have no extremely contorted
    shapes.) Justify the claim that there would be \( 10^{10000} \) or
    so valid districtings under this simple assumption.
\end{exercise}

\begin{solution}
    Assuming precincts are approximately equal in size, and the only
    constraint on valid districting is the districts have roughly equal
    population, then each district would have \( 500 \) precincts.  The way
    to allocate \( 9000 \) blocks in \( 18 \) groups of \( 500 \) each
    is the multinomial
    \[
        N = \frac{9000!}{(500!)^{18}}
    \] With Stirling's approximation, this is approximately
    \begin{align*}
        N       &\approx \frac{\sqrt{2\pi} (9000)^{9000} \sqrt{9000}
        \EulerE^{-9000}} {(\sqrt{2\pi} (500)^{500} \sqrt{500} \EulerE^{-500})^
        {18}} \\
        &= (2\pi)^{-17/2} \left( \frac{9000}{500} \right)^{9000} \left(
        \frac{\sqrt{9000}}{500^{9}} \right) \\
        &= (2\pi)^{-17/2} 18^{9000} \left( \frac{\sqrt{9000}}{500^{9}}
        \right).  \\
    \end{align*}
    Since all that is needed is the exponent, evaluate the base-\( 10 \)
    logarithm of the last expression.  That is \( \log_{10} N \approx 11
    {,}268 \).  So the assertion there are \( 10^{10000} \) or so equal
    population districtings is a justified lower estimate.

    Just for comparison, scientists estimate there are about \( 10^{80} \)
    \emph{atoms} in the universe.
\end{solution}

\begin{exercise}
    Cite a reference to a theorem from classical probability for the
    probability an \( n \)-step \( 1 \)-dimensional random walk from \(
    X_0 \) takes a first step to the right and does not return to the
    origin.
\end{exercise}
\begin{solution}
    This is Corollary 10.4, page 63 of \textit{Heads or Tails:  An
    Introduction to Limit Theorems in Probability}, Emmanuel Lesigne,
    AMS Student Mathematical Library, Volume 28, 2005.  This is also
    formulas (3.5) and (2.3), on pages 77 and 75 respectively in W.
    Feller's \textit{An Introduction to Probability Theory and Its
    Applications, Volume I.}, John Wiley and Sons, 1973.
\end{solution}

\begin{exercise}
    Verify
    \[
        \frac{1}{2^{n+1}}\binom{n}{n/2} \asympt \frac{1}{\sqrt{2 \pi n}}.
    \]
\end{exercise}
\begin{solution}
    Using Stirling's Approximation is one way, slightly tedious, to show
    this.

    Another way is to start from the inequality
    \[
        \frac{4^m}{\sqrt{(2m+1)(\PI/2)}}\left(1-\frac{1}{2m} \right) < {\binom
        {2m}{m}} < \frac{4^m}{\sqrt{(2m+1)(\PI/2)}} \left(1 + \frac{1}{2m}
        \right).
    \] established in the subsection \emph{Wallis' Formula and the
    Central Binomial Coefficient} of the section \emph{Wallis Formula}
    in the chapter on Stirling's Formula.  Rewrite \( n = 2m \) and
    divide through by \( 2^{n + 1} = 2 \cdot 4^m \) to obtain
    \[
        \frac{1}{\sqrt{2\pi(n+1)}} \left( 1 - \frac{1}{n} \right) <
        \frac{1}{2^{n+1}} \binom{n}{n/2} < \frac{1}{\sqrt{2\pi(n+1)}}
        \left( 1 + \frac{1}{n} \right).
    \] Then it quickly follows
    \[
        \frac{1}{2^{n+1}}\binom{n}{n/2} \asympt \frac{1}{\sqrt{2 \pi n}}.
    \]
\end{solution}

\hr

\visual{Books}{../../../../CommonInformation/Lessons/books.png}
\section*{Reading Suggestion:}

\bibliography{../../../../CommonInformation/bibliography}

%   \begin{enumerate}
%     \item
%     \item
%     \item
%   \end{enumerate}

\hr

\visual{Links}{../../../../CommonInformation/Lessons/chainlink.png}
\section*{Outside Readings and Links:}
\begin{enumerate}
    \item
    \item
    \item
    \item
\end{enumerate}

\section*{\solutionsname} \loadSolutions

\hr

\mydisclaim \myfooter

Last modified:  \flastmod

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-master: t
%%% TeX-master: t
%%% End:
